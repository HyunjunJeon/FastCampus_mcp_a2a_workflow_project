"""A2A í†µí•©ì´ ì ìš©ëœ í”Œë˜ë„ˆ ì—ì´ì „íŠ¸.

ì´ ëª¨ë“ˆì€ í‘œì¤€í™”ëœ A2A ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•˜ì—¬ ì‘ì—… ê³„íš ìˆ˜ë¦½ê³¼ ë°°ë¶„ì„
ìˆ˜í–‰í•˜ëŠ” í”Œë˜ë„ˆ ì—ì´ì „íŠ¸ë¥¼ ì œê³µí•œë‹¤.
"""

import asyncio
import os

from datetime import datetime
from typing import Any
from uuid import uuid4

import pytz
import structlog
import uvicorn

from a2a.types import AgentCard
from langchain_core.language_models import BaseChatModel
from langgraph.checkpoint.base import BaseCheckpointSaver
from langgraph.checkpoint.memory import InMemorySaver
from starlette.requests import Request
from starlette.responses import JSONResponse
from starlette.routing import Route

from src.a2a_integration.a2a_lg_server_utils import (
    build_a2a_starlette_application,
    build_request_handler,
    create_agent_card,
    create_agent_skill,
)
from src.a2a_integration.executor import LangGraphAgentExecutor
from src.agents.planner.planner_agent_lg import (
    create_planner_agent,
)
from src.base.a2a_interface import A2AOutput, A2AStreamBuffer, BaseA2AAgent
from src.base.base_graph_agent import BaseGraphAgent


logger = structlog.get_logger(__name__)


class PlannerA2AAgent(BaseA2AAgent, BaseGraphAgent):
    """A2A í†µí•© í”Œë˜ë„ˆ ì—ì´ì „íŠ¸.

    ì´ ì—ì´ì „íŠ¸ëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•œë‹¤:
    - ì‚¬ìš©ì ìš”ì²­ì„ êµ¬ì¡°í™”ëœ PRDë¡œ íŒŒì‹±
    - ì˜ì¡´ì„±ì„ í¬í•¨í•œ ì‹¤í–‰ ê³„íš ìƒì„±
    - ì ì ˆí•œ ì—ì´ì „íŠ¸ë¡œ ì‘ì—… ë¶„ë°°
    - ì‘ì—… ì‹¤í–‰ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§

    ìµœì¢…ì ìœ¼ë¡œ Supervisor ì—ì´ì „íŠ¸ì— êµ¬ì¡°í™”ëœ ì‘ì—… ê³„íšì„ ì œê³µí•œë‹¤.
    """

    def __init__(
        self,
        model: BaseChatModel | None = None,
        is_debug: bool = False,
        check_pointer: InMemorySaver | None = None,
    ) -> None:
        """Initialize Planner A2A Agent.

        Args:
            model: LLM model (default: gpt-4o-mini)
            is_debug: Debug mode flag
            check_pointer: Checkpoint manager (default: MemorySaver)
        """
        BaseA2AAgent.__init__(self)

        # Initialize the model and checkpointer for later use
        self.model = model
        self.check_pointer = check_pointer
        self.is_debug = is_debug
        self.agent = None  # Will be initialized async in _ensure_agent

        self.stream_buffer = A2AStreamBuffer(max_size=100)

        self.current_plan = []
        self.plan_metadata = {}
        self.workflow_phase = 'idle'

    async def initialize(self) -> bool:
        """Initialize the planner agent asynchronously."""
        try:
            await self._ensure_agent()
            return True
        except Exception as e:
            logger.error(f"Error initializing PlannerA2AAgent: {e}")
            return False

    async def _ensure_agent(self) -> None:
        """Ensure agent is initialized."""
        if self.agent is None:
            self.agent = await create_planner_agent(
                model=self.model,
                is_debug=self.is_debug,
                checkpointer=self.check_pointer,
            )

    async def execute_for_a2a(
        self, input_dict: dict[str, Any], config: dict[str, Any] | None = None
    ) -> A2AOutput:
        """Execute the agent with A2A-compatible input and output.

        Args:
            input_dict: Input data containing messages or planning request
            config: Optional configuration (thread_id, etc.)

        Returns:
            A2AOutput: Standardized output for A2A processing
        """
        try:
            # Ensure agent is initialized
            await self._ensure_agent()
            if self.agent is None:
                raise RuntimeError('Planner agent is not initialized')

            self.current_plan = []
            self.plan_metadata = {}
            self.workflow_phase = 'parsing'

            # Ensure config thread_id (use provided, or conversation_id fallback)
            if not config:
                config = {}
            config['configurable'] = config.get('configurable', {})
            if 'thread_id' not in config['configurable']:
                conv_id = input_dict.get('conversation_id') or input_dict.get('context_id')
                config['configurable']['thread_id'] = conv_id if conv_id else str(uuid4())

            result = await self.agent.ainvoke(
                input_dict,
                config=config,
            )

            logger.info(f'[PlannerA2AAgent] result: {result}')

            # Extract final output
            return self.extract_final_output(result)

        except Exception as e:
            return self.format_error(e, context='execute_for_a2a')

    def format_stream_event(self, event: dict[str, Any]) -> A2AOutput | None:
        """Convert a streaming event to standardized A2A output.

        Args:
            event: Raw streaming event from LangGraph

        Returns:
            A2AOutput if the event should be forwarded, None otherwise
        """
        event_type = event.get('event', '')

        # Handle LLM streaming
        if event_type == 'on_llm_stream':
            content = self.extract_llm_content(event)
            if content:
                # Track planning progress mentioned in content
                self._track_planning_progress(content)

                if self.stream_buffer.add(content):
                    # Buffer is full, flush it
                    return self.create_a2a_output(
                        status='working',
                        text_content=self.stream_buffer.flush(),
                        stream_event=True,
                        metadata={
                            'event_type': 'llm_stream',
                            'planning_progress': self._get_planning_progress(),
                        },
                    )

        # Handle tool execution events
        elif event_type == 'on_tool_start':
            tool_name = event.get('name', 'unknown')
            task_type = self._identify_task_type(tool_name)

            return self.create_a2a_output(
                status='working',
                text_content=f'ğŸ“ ê³„íš ì‘ì—… ì§„í–‰ ì¤‘: {task_type}',
                stream_event=True,
                metadata={
                    'event_type': 'tool_start',
                    'tool_name': tool_name,
                    'task_type': task_type,
                },
            )

        # Handle tool completion with planning results
        elif event_type == 'on_tool_end':
            tool_output = event.get('data', {}).get('output', {})
            tool_name = event.get('name', 'unknown')
            task_type = self._identify_task_type(tool_name)

            if tool_output and isinstance(tool_output, dict):
                # Update plan if parsing or expanding tasks
                if (
                    'parse' in tool_name.lower()
                    or 'expand' in tool_name.lower()
                ):
                    if 'tasks' in tool_output:
                        self.current_plan.extend(tool_output['tasks'])
                    elif 'subtasks' in tool_output:
                        self.current_plan.extend(tool_output['subtasks'])

                return self.create_a2a_output(
                    status='working',
                    data_content={
                        'plan_update': {
                            'task_type': task_type,
                            'result': tool_output,
                        },
                        'current_progress': self._get_planning_progress(),
                    },
                    stream_event=True,
                    metadata={'event_type': 'plan_update'},
                )

        # Handle completion events
        elif self.is_completion_event(event):
            # Flush any remaining buffer content
            if self.stream_buffer.has_content():
                return self.create_a2a_output(
                    status='working',
                    text_content=self.stream_buffer.flush(),
                    stream_event=True,
                    metadata={'event_type': 'buffer_flush'},
                )

        return None

    def extract_final_output(self, state: dict[str, Any]) -> A2AOutput:
        """Extract final output from the agent's state.

        Args:
            state: Final state from the LangGraph execution

        Returns:
            A2AOutput: Final standardized output
        """
        try:
            # Extract messages from state
            messages = state.get('messages', [])

            # Get the last AI message as planning summary
            planning_summary = ''
            for msg in reversed(messages):
                if (
                    hasattr(msg, 'content')
                    and msg.__class__.__name__ == 'AIMessage'
                ):
                    planning_summary = msg.content
                    break

            # Extract plan from state
            plan = state.get('plan', [])
            metadata = state.get('metadata', {})
            agent_assignments = state.get('agent_assignments', {})
            workflow_phase = state.get('workflow_phase', 'completed')

            # Prepare structured data
            data_content = {
                'success': True,
                'result': {
                    'plan': plan,
                    'metadata': metadata,
                    'agent_assignments': agent_assignments,
                    'workflow_phase': workflow_phase,
                    'summary': planning_summary,
                    'timestamp': datetime.now(pytz.UTC).isoformat(),
                },
                'agent_type': 'PlannerA2AAgent',
                'workflow_status': workflow_phase,
            }

            # Create final output
            return self.create_a2a_output(
                status='completed',
                text_content=planning_summary or 'ê³„íšì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
                data_content=data_content,
                final=True,
                metadata={
                    'execution_complete': True,
                    'total_tasks': len(plan),
                    'workflow_phase': workflow_phase,
                },
            )

        except Exception as e:
            logger.error(f'Error extracting final output: {e}')
            return self.format_error(e, context='extract_final_output')

    # Helper methods for planning

    def _track_planning_progress(self, content: str) -> None:
        """Track planning progress from content."""
        content_lower = content.lower()

        phase_keywords = {
            'parsing': ['íŒŒì‹±', 'ë¶„ì„', 'ìš”êµ¬ì‚¬í•­', 'prd'],
            'planning': ['ê³„íš', 'ì‘ì—…', 'íƒœìŠ¤í¬', 'ë‹¨ê³„'],
            'distributing': ['ë¶„ë°°', 'í• ë‹¹', 'ë°°í¬'],
            'monitoring': ['ëª¨ë‹ˆí„°ë§', 'ì¶”ì ', 'ì§„í–‰'],
        }

        for phase, keywords in phase_keywords.items():
            if any(keyword in content_lower for keyword in keywords):
                self.workflow_phase = phase

    def _identify_task_type(self, tool_name: str) -> str:
        """Identify which task type a tool represents."""
        tool_lower = tool_name.lower()

        # ì‘ì—… íƒ€ì… ë§¤í•‘
        task_mappings = {
            'parse': 'PRD íŒŒì‹±',
            'prd': 'PRD íŒŒì‹±',
            'expand': 'ì‘ì—… í™•ì¥',
            'complexity': 'ë³µì¡ë„ ë¶„ì„',
            'analyze': 'ë³µì¡ë„ ë¶„ì„',
            'create': 'ì‘ì—… ìƒì„±',
            'update': 'ìƒíƒœ ì—…ë°ì´íŠ¸',
            'get': 'ì‘ì—… ì¡°íšŒ',
            'search': 'ì‘ì—… ì¡°íšŒ',
        }

        # ë§¤í•‘ì—ì„œ ì¼ì¹˜í•˜ëŠ” í‚¤ë¥¼ ì°¾ì•„ ë°˜í™˜
        for keyword, task_type in task_mappings.items():
            if keyword in tool_lower:
                return task_type

        return 'ì¼ë°˜ ì‘ì—…'

    def _get_planning_progress(self) -> dict[str, Any]:
        """Get current planning progress."""
        return {
            'tasks_created': len(self.current_plan),
            'workflow_phase': self.workflow_phase,
            'metadata': self.plan_metadata,
        }

    def get_agent_card(self, url: str) -> AgentCard:
        """A2A AgentCard ìƒì„±.

        AgentCardëŠ” ì—ì´ì „íŠ¸ì˜ ë©”íƒ€ë°ì´í„°ì™€ ê¸°ëŠ¥ì„ ì„¤ëª…í•˜ëŠ” í‘œì¤€í™”ëœ ë¬¸ì„œì…ë‹ˆë‹¤.
        ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë‚˜ ì‹œìŠ¤í…œì´ ì´ ì—ì´ì „íŠ¸ì˜ ê¸°ëŠ¥ì„ ì´í•´í•˜ê³  ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

        Args:
            url: ì—ì´ì „íŠ¸ ì„œë²„ì˜ ê¸°ë³¸ URL

        Returns:
            AgentCard: ì—ì´ì „íŠ¸ ë©”íƒ€ë°ì´í„° ì¹´ë“œ
        """
        # Docker í™˜ê²½ì—ì„œëŠ” ì„œë¹„ìŠ¤ ì´ë¦„ì„ ì‚¬ìš©í•˜ì—¬ ë‚´ë¶€ í†µì‹ 
        if os.getenv("IS_DOCKER", "false").lower() == "true":
            url = f"http://planner-agent:{os.getenv('AGENT_PORT', '8001')}"

        skills = [
            create_agent_skill(
                skill_id="create-plan",
                name="ê³„íš ìƒì„±",
                description="ë³µì¡í•œ ì‘ì—…ì„ ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹¨ê³„ë¡œ ë¶„í•´í•˜ê³  ê³„íšì„ ìƒì„±í•©ë‹ˆë‹¤.",
                tags=["planning", "tasks", "dependencies", "orchestration"],
                examples=[
                    "ë°ì´í„° ë¶„ì„ ì›Œí¬í”Œë¡œìš° ê³„íšì„ ìƒì„±í•´ì£¼ì„¸ìš”",
                    "íŠ¸ë ˆì´ë”© ì „ëµ êµ¬í˜„ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”"
                ],
            ),
            create_agent_skill(
                skill_id="expand-task",
                name="ì‘ì—… í™•ì¥",
                description="ë³µì¡í•œ ì‘ì—…ì„ ì‹¤í–‰ ê°€ëŠ¥í•œ í•˜ìœ„ ì‘ì—…ìœ¼ë¡œ í™•ì¥í•©ë‹ˆë‹¤.",
                tags=["planning", "expansion", "decomposition"],
                examples=[
                    "'ì‹œì¥ ë°ì´í„° ë¶„ì„'ì„ í•˜ìœ„ ì‘ì—…ìœ¼ë¡œ í™•ì¥í•´ì£¼ì„¸ìš”",
                    "'íŠ¸ë ˆì´ë”© ì „ëµ êµ¬í˜„'ì„ ë‹¨ê³„ë³„ë¡œ ë‚˜ëˆ ì£¼ì„¸ìš”"
                ],
            ),
        ]

        logger.info("A2A agent card created")

        return create_agent_card(
            name="PlannerAgent",
            description="ì‘ì—… ê³„íš ìˆ˜ë¦½ ë° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì„ ìœ„í•œ Agentì…ë‹ˆë‹¤.",
            url=url,
            skills=skills,
        )


# Factory function for backward compatibility
async def create_planner_a2a_agent(
    model: BaseChatModel | None = None, is_debug: bool = False, checkpointer: BaseCheckpointSaver | None = None
) -> PlannerA2AAgent:
    """Create and initialize a Planner A2A Agent.

    Args:
        model: LLM model (default: gpt-4o-mini)
        is_debug: Debug mode flag
        checkpointer: Checkpoint manager

    Returns:
        PlannerA2AAgent: Initialized agent instance
    """
    agent = PlannerA2AAgent(model, is_debug, checkpointer)
    await agent.initialize()
    return agent


def main() -> None:
    """PlannerAgent A2A ì„œë²„ ì‹¤í–‰.

    ì´ í•¨ìˆ˜ëŠ” ì„œë²„ ì‹¤í–‰ì˜ ì§„ì…ì ìœ¼ë¡œ, ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
    1. ë¡œê¹… ì„¤ì •
    2. ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤í–‰
    3. í™˜ê²½ ì„¤ì • ë¡œë“œ
    4. A2A ì„œë²„ ìƒì„± ë° ì‹¤í–‰
    """

    async def async_init() -> "PlannerA2AAgent":
        """ë¹„ë™ê¸° ì´ˆê¸°í™” í—¬í¼ í•¨ìˆ˜.

        MCP ì„œë²„ì™€ì˜ ë¹„ë™ê¸° ì—°ê²°ì´ í•„ìš”í•˜ë¯€ë¡œ ë³„ë„ì˜ ë¹„ë™ê¸° í•¨ìˆ˜ë¡œ ë¶„ë¦¬.

        Returns:
            PlannerA2AAgent: ì´ˆê¸°í™”ëœ A2A ë˜í¼ ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” None
        """
        try:
            # PlannerA2AAgent ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”)
            _a2a_wrapper = PlannerA2AAgent(is_debug=True)

            # ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤í–‰ ë° ê²°ê³¼ í™•ì¸
            if not await _a2a_wrapper.initialize():
                logger.error("âŒ PlannerAgentA2A ì´ˆê¸°í™” ì‹¤íŒ¨")
                return None

            logger.info("âœ… PlannerAgentA2A ì´ˆê¸°í™” ì™„ë£Œ")
            return _a2a_wrapper

        except Exception as e:
            # ì´ˆê¸°í™” ì¤‘ ë°œìƒí•œ ì˜ˆì™¸ ì²˜ë¦¬
            logger.error(f"ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return None

    a2a_agent = asyncio.run(async_init())

    # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ì¡°ê¸° ì¢…ë£Œ
    if a2a_agent is None:
        return

    try:
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„œë²„ ì„¤ì • ë¡œë“œ
        # Docker í™˜ê²½ ì—¬ë¶€ í™•ì¸ - Dockerì—ì„œëŠ” ëª¨ë“  ì¸í„°í˜ì´ìŠ¤ì—ì„œ ìˆ˜ì‹ 
        is_docker = os.getenv("IS_DOCKER", "false").lower() == "true"

        # í˜¸ìŠ¤íŠ¸ ì„¤ì •: DockerëŠ” 0.0.0.0, ë¡œì»¬ì€ localhost
        host = os.getenv("AGENT_HOST", "localhost" if not is_docker else "0.0.0.0")
        port = int(os.getenv("AGENT_PORT", "8001"))
        url = f"http://{host}:{port}"

        agent_card = a2a_agent.get_agent_card(url)

        # LangGraphAgentExecutorë¡œ ë˜í•‘
        executor = LangGraphAgentExecutor(
            agent_class=PlannerA2AAgent,
            is_debug=True
        )

        # A2A ì„œë²„ ìƒì„±
        handler = build_request_handler(executor)
        server_app = build_a2a_starlette_application(
            agent_card=agent_card,
            handler=handler
        )

        # Health ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ (Starlette ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì§ì ‘ ì£¼ì…)
        app = server_app.build()

        async def health_check(request: Request) -> JSONResponse:  # type: ignore[unused-argument]
            return JSONResponse({
                'status': 'healthy',
                'agent': 'PlannerAgent',
                'timestamp': datetime.now(pytz.UTC).isoformat(),
            })

        app.router.routes.append(
            Route('/health', health_check, methods=['GET'])
        )

        # ì„œë²„ ì‹œì‘ ì •ë³´ ë¡œê¹…
        logger.info(f"âœ… PlannerAgent A2A server starting at {url} with CORS enabled")
        logger.info(f"ğŸ“‹ Agent Card URL: {url}/.well-known/agent-card.json")  # A2A í‘œì¤€ ë©”íƒ€ë°ì´í„° ì—”ë“œí¬ì¸íŠ¸
        logger.info(f"ğŸ©º Health Check: {url}/health")  # í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸

        # uvicorn ì„œë²„ ì§ì ‘ ì‹¤í–‰
        config = uvicorn.Config(
            app,
            host=host,
            port=port,
            log_level="info",
            access_log=False,
            reload=False,
            timeout_keep_alive=1000,
            timeout_notify=1000,
            ws_ping_interval=30,
            ws_ping_timeout=60,
            limit_max_requests=None,
            timeout_graceful_shutdown=10,
        )
        server = uvicorn.Server(config)
        server.run()

    except Exception as e:
        # ì„œë²„ ì‹œì‘ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë¡œê¹… ë° ì˜ˆì™¸ ì¬ë°œìƒ
        logger.error(f"ì„œë²„ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise
